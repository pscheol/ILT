# Kafka Producer & Consumer API
- producer API는 응용 프로그램에서 Kafka 클러스터로 데이터 스트림을 보낼 수 있다.

- Consumer API는 응용프로그램에서 Kafka 클러스터에서 topics에 대하여 데이터 스트림으로 받을 수 있다.

kafka consumer, producer 를 자바로 구현하려면 kafka-clients 라이브러리가 필요하다.
```shell
## maven
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>2.2.1</version>
</dependency>

## gradle
compile group: 'org.apache.kafka', name: 'kafka-clients', version: '2.2.1'

```



 * kafka client는 kafka cluster에 있는 레코드들을 기록한다.
 * Producer는 스레드로부터 safe하고, 스레드들을 통해 단일 producer 인스턴스를 공유하는 것이 다중 인스턴스를 갖는거보다 빠르다.


##### Producer 코드
```java
package com.study.kafka;

import java.util.Properties;
import java.util.Scanner;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;

public class Producer {

	public static void main(String[] args) throws Exception {
		kafkaProducer();
	}

	private static void kafkaProducer() throws Exception {
		Properties props = new Properties();
		props.put("bootstrap.servers", "localhost:9092");
		props.put("acks", "all");
		props.put("block.on.buffer.full", "true");
		props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
		props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

		KafkaProducer<String, String> producer = new KafkaProducer<String, String>(props);
		Scanner scan = new Scanner(System.in);
		while (true) {
			System.out.print("In > ");
			String msg = scan.nextLine();
			if (msg.equals("END"))
				break;

			ProducerRecord<String, String> recording = new ProducerRecord<String, String>("hello-kafka", "uMsg", msg);
			try {
				producer.send(recording);
			} catch (Exception e) {

			} finally {
				producer.flush();
			}
		}
		producer.close();
	}
}
```


##### Consumer 코드
```java

import java.time.Duration;
import java.util.Arrays;
import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;

public class Consumer {
	public static void main(String[] args) throws Exception {
		kafkaConsumer();
	}


	private static void kafkaConsumer() throws Exception {
		Properties props = new Properties();
		props.put("bootstrap.servers", "localhost:9092");
		props.setProperty("group.id", "test");
		props.setProperty("enable.auto.commit", "true");
		props.setProperty("auto.commit.interval.ms", "1000");
		props.setProperty("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
		props.setProperty("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

		KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);

		consumer.subscribe(Arrays.asList("hello-kafka"));
		while (true) {
			ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
			for (ConsumerRecord<String, String> record : records) {
				System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
			}
		}
	}
}
```

##### 결과
![](data/producer-consumer-java.png)
